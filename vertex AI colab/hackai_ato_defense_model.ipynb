{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install protobuf==3.20.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUlUBTbSlvdM",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1764500218518,
          "user_tz": -330,
          "elapsed": 6665,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "ed7e596d-3fd1-4950-c905-9f878db5ab7f"
      },
      "id": "tUlUBTbSlvdM",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: protobuf==3.20.3 in /usr/local/lib/python3.12/dist-packages (3.20.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "id": "MIYudcPb6EzK3rdBU8vyL2Ov",
      "metadata": {
        "tags": [],
        "id": "MIYudcPb6EzK3rdBU8vyL2Ov",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1764500225415,
          "user_tz": -330,
          "elapsed": 1491,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "058165f8-5a8a-4102-b224-4de5759f4401"
      },
      "source": [
        "# generate_data.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pickle\n",
        "import os\n",
        "from uuid import uuid4\n",
        "\n",
        "DATA_DIR = \"data\"\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "NUM_RECORDS = 20000\n",
        "N_FEATURES = 25\n",
        "FRAUD_RATE = 0.03   # 3% fraud\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 1. REALISTIC FEATURE DEFINITIONS\n",
        "# ---------------------------------------------\n",
        "\n",
        "def generate_normal_behavior(n):\n",
        "    return {\n",
        "        \"time_since_last_login\": np.random.gamma(3, 20, n),\n",
        "        \"failed_attempts\": np.random.poisson(0.2, n),\n",
        "        \"session_length_sec\": np.random.normal(300, 50, n),\n",
        "        \"ip_risk_score\": np.random.normal(0.1, 0.05, n),\n",
        "        \"device_change_flag\": np.random.binomial(1, 0.05, n),\n",
        "        \"browser_change_flag\": np.random.binomial(1, 0.03, n),\n",
        "        \"click_rate\": np.random.normal(2.0, 0.5, n),\n",
        "        \"scroll_depth\": np.random.normal(0.6, 0.1, n),\n",
        "        \"typing_speed_wpm\": np.random.normal(40, 5, n),\n",
        "        \"distance_from_last_location_km\": np.random.exponential(2, n),\n",
        "        \"country_risk_score\": np.random.normal(0.05, 0.02, n),\n",
        "        \"pressure_variation\": np.random.normal(0.2, 0.05, n),\n",
        "        \"touch_speed\": np.random.normal(0.3, 0.07, n),\n",
        "        \"mouse_travel_distance\": np.random.normal(200, 50, n),\n",
        "        \"hour_of_day\": np.random.randint(0, 24, n),\n",
        "        \"day_of_week\": np.random.randint(0, 7, n),\n",
        "        \"cpu_usage_pct\": np.random.normal(20, 5, n),\n",
        "        \"ram_usage_pct\": np.random.normal(30, 5, n),\n",
        "        \"network_latency_ms\": np.random.normal(50, 10, n),\n",
        "        \"num_active_sessions\": np.random.poisson(1, n),\n",
        "        \"past_fraud_attempts\": np.random.poisson(0.1, n),\n",
        "        \"velocity_login_per_hr\": np.random.normal(3, 1, n),\n",
        "        \"avg_transaction_value\": np.random.normal(1000, 200, n),\n",
        "        \"risk_from_history\": np.random.normal(0.1, 0.03, n),\n",
        "    }\n",
        "\n",
        "def generate_fraud_behavior(n):\n",
        "    return {\n",
        "        \"time_since_last_login\": np.random.exponential(1, n),\n",
        "        \"failed_attempts\": np.random.poisson(3, n),\n",
        "        \"session_length_sec\": np.random.normal(30, 10, n),\n",
        "        \"ip_risk_score\": np.random.normal(0.9, 0.1, n),\n",
        "        \"device_change_flag\": np.random.binomial(1, 0.8, n),\n",
        "        \"browser_change_flag\": np.random.binomial(1, 0.7, n),\n",
        "        \"click_rate\": np.random.normal(10, 2, n),\n",
        "        \"scroll_depth\": np.random.normal(0.2, 0.05, n),\n",
        "        \"typing_speed_wpm\": np.random.normal(90, 20, n),\n",
        "        \"distance_from_last_location_km\": np.random.exponential(200, n),\n",
        "        \"country_risk_score\": np.random.normal(0.9, 0.05, n),\n",
        "        \"pressure_variation\": np.random.normal(0.05, 0.02, n),\n",
        "        \"touch_speed\": np.random.normal(0.8, 0.1, n),\n",
        "        \"mouse_travel_distance\": np.random.normal(20, 10, n),\n",
        "        \"hour_of_day\": np.random.randint(0, 24, n),\n",
        "        \"day_of_week\": np.random.randint(0, 7, n),\n",
        "        \"cpu_usage_pct\": np.random.normal(70, 15, n),\n",
        "        \"ram_usage_pct\": np.random.normal(80, 10, n),\n",
        "        \"network_latency_ms\": np.random.normal(300, 40, n),\n",
        "        \"num_active_sessions\": np.random.poisson(5, n),\n",
        "        \"past_fraud_attempts\": np.random.poisson(2, n),\n",
        "        \"velocity_login_per_hr\": np.random.normal(20, 5, n),\n",
        "        \"avg_transaction_value\": np.random.normal(5000, 1000, n),\n",
        "        \"risk_from_history\": np.random.normal(0.8, 0.1, n),\n",
        "    }\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 2. CREATE DATASET\n",
        "# ---------------------------------------------\n",
        "num_fraud = int(NUM_RECORDS * FRAUD_RATE)\n",
        "num_normal = NUM_RECORDS - num_fraud\n",
        "\n",
        "normal_data = generate_normal_behavior(num_normal)\n",
        "fraud_data = generate_fraud_behavior(num_fraud)\n",
        "\n",
        "df_normal = pd.DataFrame(normal_data)\n",
        "df_normal[\"is_fraud\"] = 0\n",
        "\n",
        "df_fraud = pd.DataFrame(fraud_data)\n",
        "df_fraud[\"is_fraud\"] = 1\n",
        "\n",
        "df = pd.concat([df_normal, df_fraud], ignore_index=True)\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 3. ADD CUSTOMER_ID (CN101 onward) + SESSION_ID\n",
        "# ---------------------------------------------\n",
        "unique_users = 2000\n",
        "\n",
        "# Create IDs CN101 → CN(100 + unique_users)\n",
        "customer_ids = [f\"CN{100 + i}\" for i in range(1, unique_users + 1)]\n",
        "\n",
        "df[\"user_id\"] = np.random.choice(customer_ids, size=len(df))\n",
        "\n",
        "# Fraud clustering: fraud users chosen from this ID list\n",
        "fraud_users = np.random.choice(customer_ids, size=30)\n",
        "df.loc[df[\"is_fraud\"] == 1, \"user_id\"] = np.random.choice(fraud_users, size=num_fraud)\n",
        "\n",
        "# Add session_id (UUID)\n",
        "df[\"session_id\"] = [str(uuid4()) for _ in range(len(df))]\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 4. SCALING (EXCLUDE IDs + label)\n",
        "# ---------------------------------------------\n",
        "feature_cols = [c for c in df.columns if c not in [\"is_fraud\", \"user_id\", \"session_id\"]]\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "df[feature_cols] = scaler.fit_transform(df[feature_cols])\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 5. SAVE OUTPUTS\n",
        "# ---------------------------------------------\n",
        "df.to_csv(os.path.join(DATA_DIR, \"full_dataset.csv\"), index=False)\n",
        "\n",
        "with open(os.path.join(DATA_DIR, \"scaler_params.pkl\"), \"wb\") as f:\n",
        "    pickle.dump({\n",
        "        \"scaler\": scaler,\n",
        "        \"feature_order\": feature_cols\n",
        "    }, f)\n",
        "\n",
        "print(\"Generated behavioural dataset (CN101 user IDs + sessions)!\")\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated behavioural dataset (CN101 user IDs + sessions)!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_and_deploy_8step.py\n",
        "import os\n",
        "import shutil\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Sequential\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "DATA_DIR = \"data\"\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "SEQUENCE_LENGTH = 8   # <-- reduced from 64\n",
        "TIME_STEPS = SEQUENCE_LENGTH\n",
        "SCALER_PARAMS_FILE = os.path.join(DATA_DIR, \"scaler_params.pkl\")\n",
        "MODEL_PATH_DIR = os.path.join(DATA_DIR, \"lstm_autoencoder_savedmodel\")\n",
        "THRESHOLD_PATH = os.path.join(DATA_DIR, \"threshold.json\")\n",
        "FULL_DATA_CSV = os.path.join(DATA_DIR, \"full_dataset.csv\")\n",
        "\n",
        "# ---------------- LOAD DATA & ARTIFACTS ----------------\n",
        "print(\"--- Loading data ---\")\n",
        "if not os.path.exists(FULL_DATA_CSV):\n",
        "    raise FileNotFoundError(f\"{FULL_DATA_CSV} not found. Run generate_data.py first.\")\n",
        "\n",
        "df = pd.read_csv(FULL_DATA_CSV)\n",
        "non_feature_cols = {\"user_id\", \"session_id\", \"is_fraud\", \"event_time\", \"timestamp\"}\n",
        "feature_cols = [c for c in df.columns if c not in non_feature_cols]\n",
        "\n",
        "if \"is_fraud\" not in df.columns:\n",
        "    raise ValueError(\"full_dataset.csv must contain an 'is_fraud' column\")\n",
        "\n",
        "N_FEATURES = len(feature_cols)\n",
        "print(f\"Detected {N_FEATURES} feature columns (using these):\\n{feature_cols}\")\n",
        "\n",
        "X_full = df[feature_cols].values\n",
        "y_full = df[\"is_fraud\"].astype(int).values\n",
        "\n",
        "# ---------------- CREATE SEQUENCES ----------------\n",
        "def create_sequences(X, y, seq_length=TIME_STEPS):\n",
        "    seqs = []\n",
        "    labels = []\n",
        "    n = len(X)\n",
        "    if n < seq_length:\n",
        "        raise ValueError(f\"Not enough rows to create a single sequence: {n} rows < seq_length {seq_length}\")\n",
        "    for i in range(n - seq_length + 1):\n",
        "        seqs.append(X[i:i + seq_length])\n",
        "        labels.append(y[i + seq_length - 1])\n",
        "    return np.array(seqs), np.array(labels)\n",
        "\n",
        "X_seq, y_seq = create_sequences(X_full, y_full, SEQUENCE_LENGTH)\n",
        "X_train = X_seq[y_seq == 0]  # train only on normal sequences\n",
        "\n",
        "print(f\"Sequences: X_seq.shape={X_seq.shape}, X_train.shape={X_train.shape}, y_seq distribution: {np.bincount(y_seq)}\")\n",
        "\n",
        "# ---------------- BUILD MODEL ----------------\n",
        "print(\"\\n--- Building LSTM autoencoder ---\")\n",
        "model = Sequential([\n",
        "    tf.keras.Input(shape=(TIME_STEPS, N_FEATURES)),\n",
        "    layers.LSTM(128, activation=\"tanh\", return_sequences=False),\n",
        "    layers.RepeatVector(TIME_STEPS),\n",
        "    layers.LSTM(128, activation=\"tanh\", return_sequences=True),\n",
        "    layers.TimeDistributed(layers.Dense(N_FEATURES))  # linear output for scaled inputs\n",
        "])\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "model.summary()\n",
        "\n",
        "# ---------------- TRAIN ----------------\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 128\n",
        "print(f\"\\n--- Training ({EPOCHS} epochs, batch_size={BATCH_SIZE}) ---\")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, X_train,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_split=0.1,\n",
        "    shuffle=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# ---------------- THRESHOLD ----------------\n",
        "print(\"\\n--- Calculating anomaly threshold ---\")\n",
        "recons = model.predict(X_seq)\n",
        "mse_array = ((X_seq - recons) ** 2).mean(axis=(1, 2))\n",
        "threshold = float(np.percentile(mse_array[y_seq == 0], 99.5))\n",
        "print(f\"Calculated threshold (99.5 percentile normal): {threshold:.8f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "m55atrwgaSvX",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1764500595055,
          "user_tz": -330,
          "elapsed": 126697,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "93d343c3-488f-4576-c449-59addaada3b7"
      },
      "id": "m55atrwgaSvX",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Loading data ---\n",
            "Detected 24 feature columns (using these):\n",
            "['time_since_last_login', 'failed_attempts', 'session_length_sec', 'ip_risk_score', 'device_change_flag', 'browser_change_flag', 'click_rate', 'scroll_depth', 'typing_speed_wpm', 'distance_from_last_location_km', 'country_risk_score', 'pressure_variation', 'touch_speed', 'mouse_travel_distance', 'hour_of_day', 'day_of_week', 'cpu_usage_pct', 'ram_usage_pct', 'network_latency_ms', 'num_active_sessions', 'past_fraud_attempts', 'velocity_login_per_hr', 'avg_transaction_value', 'risk_from_history']\n",
            "Sequences: X_seq.shape=(19993, 8, 24), X_train.shape=(19394, 8, 24), y_seq distribution: [19394   599]\n",
            "\n",
            "--- Building LSTM autoencoder ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m78,336\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ repeat_vector_2 (\u001b[38;5;33mRepeatVector\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │       \u001b[38;5;34m131,584\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_2              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m24\u001b[0m)          │         \u001b[38;5;34m3,096\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">78,336</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ repeat_vector_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RepeatVector</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_2              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,096</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m213,016\u001b[0m (832.09 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">213,016</span> (832.09 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m213,016\u001b[0m (832.09 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">213,016</span> (832.09 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training (20 epochs, batch_size=128) ---\n",
            "Epoch 1/20\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 46ms/step - loss: 0.0347 - val_loss: 0.0202\n",
            "Epoch 2/20\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - loss: 0.0193 - val_loss: 0.0190\n",
            "Epoch 3/20\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - loss: 0.0181 - val_loss: 0.0171\n",
            "Epoch 4/20\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 43ms/step - loss: 0.0163 - val_loss: 0.0161\n",
            "Epoch 5/20\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - loss: 0.0148 - val_loss: 0.0139\n",
            "Epoch 6/20\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - loss: 0.0136 - val_loss: 0.0134\n",
            "Epoch 7/20\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - loss: 0.0130 - val_loss: 0.0133\n",
            "Epoch 8/20\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - loss: 0.0126 - val_loss: 0.0126\n",
            "Epoch 9/20\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - loss: 0.0122 - val_loss: 0.0121\n",
            "Epoch 10/20\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - loss: 0.0119 - val_loss: 0.0117\n",
            "Epoch 11/20\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - loss: 0.0114 - val_loss: 0.0113\n",
            "Epoch 12/20\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 40ms/step - loss: 0.0111 - val_loss: 0.0110\n",
            "Epoch 13/20\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - loss: 0.0107 - val_loss: 0.0106\n",
            "Epoch 14/20\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - loss: 0.0102 - val_loss: 0.0102\n",
            "Epoch 15/20\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - loss: 0.0099 - val_loss: 0.0099\n",
            "Epoch 16/20\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 0.0095 - val_loss: 0.0094\n",
            "Epoch 17/20\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 40ms/step - loss: 0.0091 - val_loss: 0.0090\n",
            "Epoch 18/20\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - loss: 0.0087 - val_loss: 0.0088\n",
            "Epoch 19/20\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - loss: 0.0083 - val_loss: 0.0085\n",
            "Epoch 20/20\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - loss: 0.0080 - val_loss: 0.0079\n",
            "\n",
            "--- Calculating anomaly threshold ---\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step\n",
            "Calculated threshold (99.5 percentile normal): 0.03476857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- EXPORT MODEL & SAVE ARTIFACTS ----------------\n",
        "print(\"\\n--- Saving artifacts ---\")\n",
        "# remove old export if exists\n",
        "if os.path.exists(MODEL_PATH_DIR):\n",
        "    shutil.rmtree(MODEL_PATH_DIR)\n",
        "\n",
        "# Keras 3 SavedModel export\n",
        "model.export(MODEL_PATH_DIR)\n",
        "\n",
        "# Save threshold\n",
        "with open(THRESHOLD_PATH, \"w\") as f:\n",
        "    json.dump({\"threshold\": threshold}, f)\n",
        "\n",
        "# If scaler exists in file (from generate_data.py), preserve it; else make simple Min/Max from data\n",
        "if os.path.exists(SCALER_PARAMS_FILE):\n",
        "    with open(SCALER_PARAMS_FILE, \"rb\") as f:\n",
        "        scaler_blob = pickle.load(f)\n",
        "    # ensure feature_order saved matches detected feature_cols\n",
        "    scaler_blob[\"feature_order\"] = feature_cols\n",
        "    with open(SCALER_PARAMS_FILE, \"wb\") as f:\n",
        "        pickle.dump(scaler_blob, f)\n",
        "    print(f\"Updated existing scaler artifact at {SCALER_PARAMS_FILE}\")\n",
        "else:\n",
        "    # compute min/max from training data (useful fallback for inference)\n",
        "    data_min = np.min(X_full, axis=0).tolist()\n",
        "    data_max = np.max(X_full, axis=0).tolist()\n",
        "    scaler_blob = {\"feature_order\": feature_cols, \"min\": data_min, \"max\": data_max}\n",
        "    with open(SCALER_PARAMS_FILE, \"wb\") as f:\n",
        "        pickle.dump(scaler_blob, f)\n",
        "    print(f\"Saved fallback scaler params at {SCALER_PARAMS_FILE}\")\n",
        "\n",
        "print(\"\\n✅ Export complete.\")\n",
        "print(f\"- Model saved to: {MODEL_PATH_DIR}\")\n",
        "print(f\"- Threshold saved to: {THRESHOLD_PATH}\")\n",
        "print(f\"- Scaler params saved to: {SCALER_PARAMS_FILE}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kMPqxOYyFh3",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1764500622916,
          "user_tz": -330,
          "elapsed": 3374,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "e1cc084c-9410-4566-fd76-022be561010d"
      },
      "id": "_kMPqxOYyFh3",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Saving artifacts ---\n",
            "Saved artifact at 'data/lstm_autoencoder_savedmodel'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 8, 24), dtype=tf.float32, name='keras_tensor_10')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 8, 24), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  135969493803728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135969493803152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135969493804496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135969493805456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135969493806608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135969493804112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135969493803920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135969493806032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "Updated existing scaler artifact at data/scaler_params.pkl\n",
            "\n",
            "✅ Export complete.\n",
            "- Model saved to: data/lstm_autoencoder_savedmodel\n",
            "- Threshold saved to: data/threshold.json\n",
            "- Scaler params saved to: data/scaler_params.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil -m cp -r data/lstm_autoencoder_savedmodel gs://account_takeover_model/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T46WympwKCfv",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1764500663429,
          "user_tz": -330,
          "elapsed": 3337,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "f524b9e2-5920-49ba-d77f-f6daa2f1b97e"
      },
      "id": "T46WympwKCfv",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying file://data/lstm_autoencoder_savedmodel/fingerprint.pb [Content-Type=application/octet-stream]...\n",
            "/ [0/4 files][    0.0 B/  1.8 MiB]   0% Done                                    \rCopying file://data/lstm_autoencoder_savedmodel/saved_model.pb [Content-Type=application/octet-stream]...\n",
            "/ [0/4 files][    0.0 B/  1.8 MiB]   0% Done                                    \rCopying file://data/lstm_autoencoder_savedmodel/variables/variables.data-00000-of-00001 [Content-Type=application/octet-stream]...\n",
            "Copying file://data/lstm_autoencoder_savedmodel/variables/variables.index [Content-Type=application/octet-stream]...\n",
            "/ [4/4 files][  1.8 MiB/  1.8 MiB] 100% Done                                    \n",
            "Operation completed over 4 objects/1.8 MiB.                                      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "PROJECT_ID = \"liquid-anchor-478906-e3\"  # replace with your project\n",
        "REGION = \"us-central1\"\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION)\n"
      ],
      "metadata": {
        "id": "AmUVgipuKCjN",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1764500691026,
          "user_tz": -330,
          "elapsed": 820,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "AmUVgipuKCjN",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gcs_model_path = \"gs://account_takeover_model/lstm_autoencoder_savedmodel/\"\n",
        "\n",
        "model = aiplatform.Model.upload(\n",
        "    display_name=\"hackai_ato_defence_model_v3\",\n",
        "    artifact_uri=gcs_model_path,  # must point to the SavedModel folder\n",
        "    serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-11:latest\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "nPj_y_R0KCmc",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1764500704597,
          "user_tz": -330,
          "elapsed": 5579,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "nPj_y_R0KCmc",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "endpoint = model.deploy(\n",
        "    machine_type=\"n1-standard-4\",\n",
        "    min_replica_count=1,\n",
        "    max_replica_count=1\n",
        ")"
      ],
      "metadata": {
        "id": "asNQ7tNKKCpg",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1764501916253,
          "user_tz": -330,
          "elapsed": 1187911,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "asNQ7tNKKCpg",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GL3ri88bKCsi"
      },
      "id": "GL3ri88bKCsi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JAIkVr1WKCwK"
      },
      "id": "JAIkVr1WKCwK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7gAMtwztKCzP"
      },
      "id": "7gAMtwztKCzP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from uuid import uuid4\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "PROJECT = \"liquid-anchor-478906-e3\"\n",
        "DATASET = \"ato_historical_Data\"\n",
        "TABLE = \"customer_footprint\"\n",
        "USER_ID = \"CN1001\"\n",
        "HIST_LIMIT = 64\n",
        "MIN_HISTORY = 20  # Minimum records for personal history\n",
        "\n",
        "client = bigquery.Client(project=PROJECT)\n",
        "\n",
        "# ---------------- 1. Pull last HIST_LIMIT behavioural records for the user ----------------\n",
        "query_user = f\"\"\"\n",
        "SELECT *\n",
        "FROM `{PROJECT}.{DATASET}.{TABLE}`\n",
        "WHERE user_id = '{USER_ID}'\n",
        "ORDER BY day_of_week DESC, hour_of_day DESC\n",
        "LIMIT {HIST_LIMIT}\n",
        "\"\"\"\n",
        "df_hist = client.query(query_user).to_dataframe()\n",
        "\n",
        "# ---------------- 2. Identify feature columns ----------------\n",
        "non_features = {\"user_id\", \"session_id\", \"is_fraud\"}\n",
        "feature_cols = [c for c in df_hist.columns if c not in non_features]\n",
        "\n",
        "# ---------------- 3. Decide whether to use user history or global stats ----------------\n",
        "if df_hist.empty or len(df_hist) < MIN_HISTORY:\n",
        "    print(f\"Not enough history for {USER_ID} ({len(df_hist)} records). Using global stats.\")\n",
        "\n",
        "    query_global = f\"\"\"\n",
        "    SELECT *\n",
        "    FROM `{PROJECT}.{DATASET}.{TABLE}`\n",
        "    LIMIT 5000\n",
        "    \"\"\"\n",
        "    df_global = client.query(query_global).to_dataframe()\n",
        "    X = df_global[feature_cols].astype(float).values\n",
        "    user_mean = X.mean(axis=0)\n",
        "    user_std = X.std(axis=0) + 1e-6\n",
        "else:\n",
        "    X = df_hist[feature_cols].astype(float).values\n",
        "    user_mean = X.mean(axis=0)\n",
        "    user_std = X.std(axis=0) + 1e-6\n",
        "\n",
        "# ---------------- 4. Create anomalous (fraudulent) behaviour vector ----------------\n",
        "deviation_factor = np.random.uniform(3, 8)  # strong fraud deviation\n",
        "fraud_vector = user_mean + deviation_factor * user_std\n",
        "\n",
        "# Clip values to reasonable ranges\n",
        "fraud_vector = np.clip(fraud_vector, -5, 10)\n",
        "\n",
        "# ---------------- 5. Construct final fraud row ----------------\n",
        "fraud_row = pd.DataFrame([{\n",
        "    **{col: fraud_vector[i] for i, col in enumerate(feature_cols)},\n",
        "    \"user_id\": USER_ID,\n",
        "    \"session_id\": str(uuid4()),\n",
        "    \"is_fraud\": 1\n",
        "}])\n",
        "\n",
        "print(\"\\nGenerated Fraudulent Record:\")\n",
        "print(fraud_row)\n",
        "\n",
        "# ---------------- 6. Optional: Insert into BigQuery ----------------\n",
        "# client.insert_rows_json(f\"{PROJECT}.{DATASET}.{TABLE}\", fraud_row.to_dict(orient=\"records\"))\n",
        "# print(\"Fraud record inserted into BigQuery\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJmpdEuU3zhU",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1764495916934,
          "user_tz": -330,
          "elapsed": 5776,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "6818f18b-5711-439b-fbd7-9953eb71d940"
      },
      "id": "BJmpdEuU3zhU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not enough history for CN1001 (15 records). Using global stats.\n",
            "\n",
            "Generated Fraudulent Record:\n",
            "   time_since_last_login  failed_attempts  session_length_sec  ip_risk_score  \\\n",
            "0               0.603166         0.201341            0.998341       0.314821   \n",
            "\n",
            "   device_change_flag  browser_change_flag  click_rate  scroll_depth  \\\n",
            "0            0.862705              0.71925     0.25356       0.96305   \n",
            "\n",
            "   typing_speed_wpm  distance_from_last_location_km  ...  ram_usage_pct  \\\n",
            "0          0.296075                        0.006139  ...       0.403951   \n",
            "\n",
            "   network_latency_ms  num_active_sessions  past_fraud_attempts  \\\n",
            "0            0.185664             0.000004             0.176167   \n",
            "\n",
            "   velocity_login_per_hr  avg_transaction_value  risk_from_history  user_id  \\\n",
            "0               0.214789               0.197129           0.216808   CN1001   \n",
            "\n",
            "                             session_id  is_fraud  \n",
            "0  3d7ada03-66cf-455e-b425-99398576effb         1  \n",
            "\n",
            "[1 rows x 27 columns]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "hsbchackai (Nov 30, 2025, 12:27:24 PM)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}